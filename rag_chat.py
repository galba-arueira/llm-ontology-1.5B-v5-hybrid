"""
v5 RAG Chat (V5 Hybrid)
============================

Sistema de chat que integra:
1. V5 Semantic Planner (para entender inten√ß√£o e extrair entidades)
2. Cypher Executor (para buscar no Neo4j)
3. Qwen-2.5-1.5B-Instruct (para formatar resposta e conversar)

Fluxo:
User Query -> Planner (Score > 0.6?)
    Sim -> Executa no Neo4j -> Contexto -> LLM -> Resposta
    N√£o -> LLM -> Resposta (conversa geral)
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from semantic_query_planner import get_planner
from cypher_executor import CypherExecutor
import json

# Configura√ß√£o
MODEL_PATH = "Qwen/Qwen2.5-1.5B-Instruct" # Modelo base, n√£o o fine-tuned
SYSTEM_PROMPT = """Voc√™ √© o assistente do sistema forense v5. 
Te enviarei 1 pergunta e 1 resposta e quero que voc√™ formate a resposta usando APENAS as informa√ß√µes que est√£o na resposta, sem adicionar informa√ß√µes que n√£o est√£o na resposta"""

class RAGChat:
    def __init__(self):
        print("üöÄ Inicializando v5 RAG System...")
        
        # 1. Planner & Executor
        self.planner = get_planner()
        self.executor = CypherExecutor()
        
        # 2. LLM
        print(f"üîÑ Carregando LLM ({MODEL_PATH})...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            device_map="auto",
            torch_dtype=torch.float16
        )
        print("‚úÖ Sistema pronto!\n")

    def generate_response(self, query, context=None):
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
        ]
        
        user_content = query
        
        if context: 
            # Converte o contexto (Neo4j) em texto leg√≠vel
            context_text = self._format_context_as_text(context)
            user_content = f"""
PERGUNTA: 
{query}

RESPOSTA:
{context_text}


"""
            
        messages.append({
            "role": "user", 
            "content": user_content
        })

        print(messages)

        # === A PARTIR DAQUI √â QUE VOC√ä REALMENTE CHAMA O LLM ===
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False  # greedy: menos aleat√≥rio, mais est√°vel para teste
            )
            
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # Limpa tokens do template do Qwen (gen√©rico, n√£o preso a placa)
        if "<|im_start|>assistant" in response:
            response = response.split("<|im_start|>assistant")[-1]
            response = response.split("<|im_end|>")[0].strip()
        elif "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1]
            response = response.split("<|end|>")[0].strip()
        
        return response
    
    def _format_context_as_text(self, context):
        """Convert Neo4j results to natural language"""
        if not context:
            return "Nenhum dado encontrado."
        
        text_parts = []
        for i, record in enumerate(context, 1):
            if not isinstance(record, dict):
                # fallback: tudo que n√£o for dict vira string bruta
                text_parts.append(f"{i}:")
                text_parts.append(f"  - raw: {record}")
                continue

            text_parts.append(f"{i}:")
            
            # Handle nested structure (e.g., {'resultado': {...}})
            data = record
            if len(record) == 1 and isinstance(list(record.values())[0], dict):
                # Unwrap single-key dict
                data = list(record.values())[0]
            
            for key, value in data.items():
                if key in ['uri', 'localName']:
                    continue  # Skip campos t√©cnicos
                
                # Handle list values
                if isinstance(value, list):
                    value = value[0] if len(value) == 1 else ", ".join(str(v) for v in value)
                
                # "bonitiza" o nome das chaves (gen√©rico: funciona pra qualquer entidade)
                key_pretty = (
                    key.replace('_', ' ')
                       .replace('personFullName', 'Nome')
                       .replace('cpf', 'CPF')
                )
                text_parts.append(f"  - {key_pretty}: {value}")
        
        return "\n".join(text_parts)


    def chat_loop(self):
        print("üí¨ Chat iniciado. Digite 'sair' para encerrar.")
        print("-" * 50)
        
        while True:
            try:
                query = input("\nüë§ Voc√™: ").strip()
                if query.lower() in ["sair", "exit", "quit"]:
                    break
                if not query:
                    continue
                
                # 1. Analisar Inten√ß√£o
                print("   Thinking...", end="\r")
                alls = self.planner.classify_intent(query)
                context = []

                best_intent, best_score = alls[0]
                print(f"   {best_intent}: {best_score}")

                if best_score > 0.55:
                    plan = self.planner.generate_plan(query)

                    if "error" not in plan:
                        results = self.executor.execute_plan(plan)

                        if isinstance(results, dict) and "error" in results:
                            print(f"   ‚ö†Ô∏è Erro na execu√ß√£o: {results['error']}")
                        elif not results:
                            print("   ‚ö†Ô∏è Nenhum dado encontrado no grafo.")
                            context.append({
                                "info": "Nenhum registro encontrado no banco de dados para esta consulta."
                            })
                        else:
                            print(f"   ‚úÖ Encontrados {len(results)} registros.")
                            # results √© uma lista de dicts -> flatten no contexto
                            context.extend(results)
                else:
                    # score baixo: n√£o √© consulta de grafo, deixa o LLM responder "livre"
                    pass
                
                # 2. Gerar Resposta
                response = self.generate_response(query, context)
                print(f"ü§ñ v5: {response}")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"‚ùå Erro: {e}")

if __name__ == "__main__":
    chat = RAGChat()
    chat.chat_loop()
